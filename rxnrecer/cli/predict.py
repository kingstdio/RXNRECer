import sys
import os
from rxnrecer.config import config as cfg
import pandas as pd
import numpy as np
import time
from types import SimpleNamespace
import torch
import argparse
from rxnrecer.lib.ml import mlpredict as predRXN
from rxnrecer.lib.llm import qa as llm_qa
from rxnrecer.utils import file_utils as ftool
from rxnrecer.utils import format_utils
from rxnrecer.utils import bio_utils as butils
from tqdm import tqdm
from collections import defaultdict
from rxnrecer.lib.model import mactive as Mactive

def load_model(model_weight_path= cfg.FILE_MOLEL_PRODUCTION_BEST_MODEL):
    mcfg = SimpleNamespace(
    # Model parameters
    batch_size=1,
    esm_out_dim=1280,
    gru_h_dim=512,
    att_dim=32,
    dropout_rate=0.2,
    freeze_esm_layers = 32, # Number of frozen layers
    output_dimensions=10479,
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    model_weight_path=model_weight_path,
    dict_path = cfg.FILE_DS_DICT_ID2RXN
    )

    print(f'Use device: {mcfg.device}')
    model = Mactive.BGRU(
        input_dimensions=mcfg.esm_out_dim,
        device = mcfg.device,
        gru_h_size=mcfg.gru_h_dim,
        attention_size=mcfg.att_dim,
        dropout=mcfg.dropout_rate,
        output_dimensions=mcfg.output_dimensions,
        freeze_esm_layers=mcfg.freeze_esm_layers,
    )
    
    return model, mcfg



# region 1. Load computation data
def load_data(input_data):
    """
    Load computation data, supports two input formats:
      1) Pass FASTA file path (str) -> auto parse to DataFrame
      2) Pass DataFrame directly -> must contain 'seq' and 'uniprot_id' columns
    """
    if isinstance(input_data, str):
        # Assume input is FASTA file path
        print(f'Detected input is a FASTA file: {input_data}')
        input_df = ftool.fasta_to_dataframe(fasta_file=input_data)
    elif isinstance(input_data, pd.DataFrame):
        input_df = input_data.copy().reset_index(drop=True)
    else:
        raise ValueError("input_data should be either a path to FASTA or a pandas DataFrame.")
    
    # Ensure required columns exist
    if 'seq' not in input_df.columns:
        raise ValueError("Input DataFrame must contain column 'seq'.")
    if 'uniprot_id' not in input_df.columns:
        raise ValueError("Input DataFrame must contain column 'uniprot_id'.")
    
    return input_df
# endregion 




#region 2. Save results
def save_data(resdf, output_file, output_format='tsv'):
    """
    Save results to file (TSV/JSON)
    resdf: Result DataFrame
    output_file: Output file path
    output_format: 'tsv' or 'json'
    """
    resdf = resdf.applymap(lambda x: butils.format_obj(x, 4))
    if output_format == 'tsv':
        resdf[["reaction_ec", "reaction_equation", "reaction_equation_ref_chebi"]] = resdf['rxn_details'].apply(butils.simplify_rxn_details_fields).apply(pd.Series)
        resdf = resdf[['input_id', 'RXNRECer', 'RXNRECer_with_prob', 'reaction_ec', 'reaction_equation', 'reaction_equation_ref_chebi']]
        resdf.to_csv(output_file, index=False, sep='\t', float_format="%.4f")
    elif output_format == 'json':
        resdf.to_json(output_file, orient='records', indent=4)
    else:
        print(f'Error: Invalid output format {output_format}. Skip saving.')
# endregion 



def step_by_step_prediction(input_data, mode='s1', batch_size=100):
    
    if mode == 's3':
        if not cfg.LLM_API_KEY or not cfg.LLM_API_URL:
            print("Error: LLM API key and URL are required for S3 mode!")
            return
    
    input_df = load_data(input_data)
    print(f'Step 1: Preparing input data, loading {len(input_df)} proteins')

    # Step 2: Load predictive model
    print(f'Step 2: Loading predictive model')
    model, mcfg = load_model(model_weight_path=cfg.FILE_MOLEL_PRODUCTION_BEST_MODEL) 
    
        
    res = []
    
    print(f'Step 3: Running prediction on {len(input_df)} proteins')
    # Total number of batches needed (rounding up for incomplete batches)
    batch_num = (len(input_df) + batch_size - 1) // batch_size
    
    for i in tqdm(range(batch_num)):
        # Calculate batch start and end indices
        start = i * batch_size
        end = min((i + 1) * batch_size, len(input_df))  # Ensure the last batch is handled correctly
        
        # Slice the input_data
        batch_data = input_df.iloc[start:end]
        
        # If the batch is non-empty, run prediction
        if not batch_data.empty:
            res_batch = single_batch_run_prediction(batch_data, model=model, mcfg=mcfg, mode=mode)
            res = res + [res_batch]  # Use extend for better performance
    fres = pd.concat(res, axis=0, ignore_index=True)
            
    fres = fres.drop_duplicates(subset=['input_id'], keep='first')
    # fres = input_df.rename(columns={'uniprot_id':'input_id'}).merge(fres, on='input_id', how='left')
    return fres
    


#region RXNRECer Prediction API
def single_batch_run_prediction(input_df, model, mcfg,  mode='s1'):
    """
    Perform RXNRECer prediction on input DataFrame and return prediction results.
    Args:
      - input_df: Input DataFrame with 'seq' and 'uniprot_id' columns
      - model: Pre-trained model
      - mcfg: Model configuration
      - mode: Prediction mode ('s1' or 's2')
    """
    input_df = input_df.reset_index(drop=True)
    # RXNRECer-S1
    print('Running RXNRECer-S1 ...')
    res, res_prob = Mactive.predict_sequences(
        model=model,
        sequences=input_df.seq,
        model_weight_path=mcfg.model_weight_path,
        dict_path=mcfg.dict_path,
        batch_size=2,
        device=mcfg.device
    )
    # Integrate prediction results
    res_df_s1 = input_df[['uniprot_id']].reset_index(drop=True).copy()
    res_df_s1['RXNRECer'] = res
    res_df_s1['RXNRECer_with_prob'] = res_prob
    res_df_s1 = res_df_s1.rename(columns={'uniprot_id': 'input_id'})
    
    if mode == 's1':
        # Get reaction details
        rxn_bank = pd.read_feather(cfg.FILE_RHEA_REACTION)
        res_df_s1= butils.get_rxn_details_batch(df_rxns=res_df_s1, rxn_bank=rxn_bank, rxn_id_column='RXNRECer')
        res_df_s1['rxn_details'] = res_df_s1.apply(lambda x: format_utils.format_rxn_output(RXNRECer_with_prob=x.RXNRECer_with_prob, RXN_details=x.RXN_details, mode='s2'), axis=1).tolist()
        res_df_s1 = res_df_s1[['input_id','RXNRECer', 'RXNRECer_with_prob', 'rxn_details']]
        return res_df_s1


    # Ensemble learning
    if mode == 's2':
        print('Running RXNRECer-S2 ...')
        res_df_s2 = get_ensemble(input_df=input_df[['uniprot_id', 'seq']], rxnrecer_df=res_df_s1)  # Ensemble learning
        # Get reaction details
        rxn_bank = pd.read_feather(cfg.FILE_RHEA_REACTION)
        res_df_s2= butils.get_rxn_details_batch(df_rxns=res_df_s2, rxn_bank=rxn_bank, rxn_id_column='RXNRECer')
        # Format output
        res_df_s2['rxn_details'] = res_df_s2.apply(lambda x: format_utils.format_rxn_output(RXNRECer_with_prob=x.RXNRECer_with_prob, RXN_details=x.RXN_details, mode='s2'), axis=1).tolist()
        res_df_s2 = res_df_s2[['input_id','RXNRECer', 'RXNRECer_with_prob', 'rxn_details']]
        return res_df_s2
    
    if mode == 's3':
        print('Running RXNRECer-S3 ...')
        res_df_s2 = get_ensemble(input_df=input_df[['uniprot_id', 'seq']], rxnrecer_df=res_df_s1).rename(columns={'RXNRECer': 'RXNRECer-S2'})
        s3_input_df = res_df_s2.merge(res_df_s1[['input_id', 'RXNRECer']].rename(columns={'RXNRECer': 'RXNRECer-S1'}), on='input_id', how='left'
                            ).merge(input_df.rename(columns={'uniprot_id': 'input_id'}), on='input_id', how='left')
        res_df_s3 = llm_qa.batch_chat(res_rxnrecer=s3_input_df, api_key=cfg.LLM_API_KEY, api_url=cfg.LLM_API_URL)
        res_df_s3['rxn_details'] = res_df_s3.apply(lambda x: format_utils.format_rxn_output(RXNRECer_with_prob=x.RXNRECer_with_prob, 
                                                                                            RXNRECER_S3=x['RXNRECER-S3'], 
                                                                                            RXN_details=x.RXN_details, mode='s3'), axis=1)
        res_df_s3 = res_df_s3[['input_id','RXNRECer-S2', 'RXNRECer_with_prob', 'rxn_details']].rename(columns={'RXNRECer-S2': 'RXNRECer'})
        
        return res_df_s3
        
# endregion

#region Handle enzyme/non-enzyme mixed cases in ensemble
def res_refinement(rxn_prob):
    """
    Refine enzyme prediction results with rules:
    1. Single result (enzyme or non-enzyme) -> keep as is
    2. If '-' has highest probability -> non-enzyme, keep '-'
    3. If '-' has lower probability -> remove '-'
    """
    if len(rxn_prob) == 1:
        return cfg.SPLITER.join(rxn_prob.keys()), rxn_prob

    sorted_items = sorted(rxn_prob.items(), key=lambda x: x[1], reverse=True)

    # If '-' has highest probability -> non-enzyme, keep
    if sorted_items[0][0] == '-':
        return '-', {'-': rxn_prob['-']}

    # If '-' exists and not highest -> remove regardless of position
    if '-' in rxn_prob:
        rxn_prob.pop('-')

    return cfg.SPLITER.join(rxn_prob.keys()), rxn_prob
# endregion


def get_ensemble(input_df, rxnrecer_df):
    """
    Ensemble multiple methods for protein reaction prediction and handle enzyme/non-enzyme mixed cases.

    Args:
    - input_df: Protein info DataFrame with 'uniprot_id' column
    - rxnrecer_df: RXNRECer prediction results with 'input_id' and 'RXNRECer_with_prob' columns

    Returns:
    - res_df: DataFrame with input_id, ensemble prediction string (RXNRECer) and probability dict (RXNRECer_with_prob)
    """

    # Call various prediction methods (MSA, CatFam, ECRECer, T5, RXNRECer)
    res_msa = predRXN.getmsa(df_test=input_df, k=1)
    res_catfam = predRXN.getcatfam(df_test=input_df)
    res_ecrecer = predRXN.getecrecer(df_test=input_df)
    res_t5 = predRXN.getT5(df_test=input_df, topk=1)
    res_rxnrecer = rxnrecer_df.copy().rename(columns={'input_id': 'uniprot_id'})

    # Merge different model results into same DataFrame (left join on uniprot_id)
    baggingdf = res_rxnrecer.merge(
                    res_msa[['uniprot_id', 'rxn_msa']], on='uniprot_id', how='left'
                ).merge(
                    res_catfam[['uniprot_id', 'rxn_catfam']], on='uniprot_id', how='left'
                ).merge(
                    res_ecrecer[['uniprot_id', 'rxn_ecrecer']], on='uniprot_id', how='left'
                ).merge(
                    res_t5, on='uniprot_id', how='left'
                )

    # Standardize missing predictions: convert NO-PREDICTION, None and NaN to '-'
    baggingdf.replace(['NO-PREDICTION', 'None'], '-', inplace=True)
    baggingdf.fillna('-', inplace=True)

    # Call ensemble method for fusion, return reaction prediction dict with probabilities for each protein
    baggingdf['ensemble'] = baggingdf.apply(lambda x: integrateEnsemble(
        esm=x.esm,
        t5=x.t5,
        rxn_recer=x.RXNRECer_with_prob,
        rxn_msa=x.rxn_msa,
        rxn_catfam=x.rxn_catfam,
        rxn_ecrecer=x.rxn_ecrecer
    ), axis=1)

    # Extract ensemble results into two columns:
    # - RXNRECer: concatenated reaction ids (string)
    # - RXNRECer_with_prob: original ensemble dict
    baggingdf['RXNRECer_with_prob'] = baggingdf['ensemble']
    baggingdf['RXNRECer'] = baggingdf['ensemble'].apply(lambda x: cfg.SPLITER.join(x.keys()))

    # Restore input_id naming for consistency with external interface
    baggingdf.rename(columns={'uniprot_id': 'input_id'}, inplace=True)

    # Keep only required output fields
    res_df = baggingdf[['input_id', 'RXNRECer', 'RXNRECer_with_prob']].copy()

    # Clean enzyme/non-enzyme mixed cases (remove invalid or conflicting '-' labels)
    res_df[['RXNRECer', 'RXNRECer_with_prob']] = res_df.apply(
        lambda row: pd.Series(res_refinement(dict(row['RXNRECer_with_prob']))),
        axis=1
    )


    return res_df
    

def integrateEnsemble(esm, t5, rxn_recer, rxn_msa, rxn_catfam, rxn_ecrecer):
    """
    æ•´åˆä¸åŒæ¥æºçš„ RHEA ID æ¦‚ç‡ï¼Œä¼˜å…ˆä¿ç•™è¾ƒé«˜çš„æ¦‚ç‡å€¼ï¼Œå¹¶å¯¹å•ä¸ª RHEA ID è¿›è¡Œå½’å¹¶ã€‚
    
    å‚æ•°:
        esm: åŒ…å« (RHEA_IDs, æ¦‚ç‡) çš„å…ƒç»„åˆ—è¡¨ï¼Œè¿™é‡Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        t5: åŒ…å« (RHEA_IDs, æ¦‚ç‡) çš„å…ƒç»„åˆ—è¡¨ï¼Œè¿™é‡Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        rxn_recer: å­—å…¸ï¼Œé”®ä¸º RHEA_IDï¼Œå€¼ä¸ºå¯¹åº”çš„æ¦‚ç‡
        rxn_msa: å­—ç¬¦ä¸²ï¼ŒMSA æ–¹æ³•äº§ç”Ÿçš„ RHEA IDï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªï¼Œç”¨åˆ†å·åˆ†éš”ï¼‰
        rxn_catfam: å­—ç¬¦ä¸²ï¼Œåˆ†ç±»å®¶æ—æ–¹æ³•äº§ç”Ÿçš„ RHEA IDï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªï¼Œç”¨åˆ†å·åˆ†éš”ï¼‰
        rxn_ecrecer: å­—ç¬¦ä¸²ï¼ŒEC æ–¹æ³•äº§ç”Ÿçš„ RHEA IDï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªï¼Œç”¨åˆ†å·åˆ†éš”ï¼‰
    
    è¿”å›:
        dict: ä»¥å•ä¸ª RHEA ID ä¸ºé”®ï¼Œå¯¹åº”æœ€å¤§æ¦‚ç‡ä¸ºå€¼çš„å­—å…¸ï¼ŒæŒ‰æ¦‚ç‡é™åºæ’åº
    """
    # -------------------------------
    # 1. èšåˆ esm ä¸ t5 çš„é¢„æµ‹ç»“æœ
    # -------------------------------
    # ä½¿ç”¨ defaultdict(list) å°†åŒä¸€ç»„ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ª RHEA IDï¼Œä»¥åˆ†å·åˆ†éš”ï¼‰çš„æ¦‚ç‡æ”¶é›†åœ¨ä¸€èµ·
    aggregated = defaultdict(list)
    for prediction in (esm[0], t5[0]):
        # å¦‚æœ RHEA ID ä¸º Noneï¼Œåˆ™æ›¿æ¢ä¸º '-' ä»¥ä¿æŒä¸€è‡´æ€§
        rhea_ids = prediction[0] if prediction[0] is not None else '-'
        aggregated[rhea_ids].append(prediction[1])
    
    # -------------------------------
    # 2. æ·»åŠ  rxn_recer çš„é¢„æµ‹ç»“æœ
    # -------------------------------
    # åŒæ ·ç¡®ä¿ None æ›¿æ¢ä¸º '-'
    for rhea_id, prob in rxn_recer.items():
        key = rhea_id if rhea_id is not None else '-'
        aggregated[key].append(prob)
    
    # -------------------------------
    # 3. è®¡ç®—æ¯ç»„ RHEA ID çš„æœ€é«˜æ¦‚ç‡
    # -------------------------------
    # aggregated çš„é”®å¯èƒ½åŒ…å«å¤šä¸ª RHEA IDï¼ˆå¦‚ "ID1;ID2"ï¼‰ï¼Œæ­¤å¤„å¯¹æ¯ç»„å–æœ€å¤§æ¦‚ç‡
    group_max_prob = {group: max(probs) for group, probs in aggregated.items()}
    
    # -------------------------------
    # 4. å°† RHEA ID ç»„æ‹†åˆ†æˆå•ä¸ª RHEA IDï¼Œå¹¶ä¿ç•™è¾ƒé«˜æ¦‚ç‡
    # -------------------------------
    direct_prob = {}
    for group, prob in group_max_prob.items():
        # æ‹†åˆ†å¯èƒ½ç”±åˆ†å·è¿æ¥çš„å¤šä¸ª RHEA ID
        for rhea_id in group.split(';'):
            # å¦‚æœåŒä¸€ RHEA ID æ¥è‡ªå¤šä¸ªç»„ï¼Œä¿ç•™æ¦‚ç‡è¾ƒå¤§çš„é‚£ä¸ª
            direct_prob[rhea_id] = max(direct_prob.get(rhea_id, 0), prob)
    
    # -------------------------------
    # 5. æ„å»º EC æ–¹æ³•æ‰‹åŠ¨æ·»åŠ çš„ RHEA ID å­—å…¸
    # -------------------------------
    # å°† rxn_msa, rxn_catfam, rxn_ecrecer ä¸‰ä¸ªå­—ç¬¦ä¸²æ‹¼æ¥åï¼Œä»¥åˆ†å·æ‹†åˆ†ï¼Œå¾—åˆ°å”¯ä¸€çš„ RHEA ID é›†åˆ
    ec_ids = set(f"{rxn_msa};{rxn_catfam};{rxn_ecrecer}".split(';'))
    # å¯¹æ¯ä¸ª EC æ–¹æ³•äº§ç”Ÿçš„ RHEA ID èµ‹äºˆå›ºå®šæ¦‚ç‡ 0.7777
    ec_prob = {rhea_id: 0.7777 for rhea_id in ec_ids}
    
    # -------------------------------
    # 6. åˆå¹¶æ¥è‡ª direct_prob ä¸ ec_prob çš„ç»“æœ
    # -------------------------------
    # å¯¹äºç›¸åŒçš„ RHEA IDï¼Œå–ä¸¤è¾¹æ¦‚ç‡ä¸­çš„è¾ƒå¤§å€¼
    merged_probs = {}
    for mapping in (ec_prob, direct_prob):
        for rhea_id, prob in mapping.items():
            merged_probs[rhea_id] = max(merged_probs.get(rhea_id, 0), prob)
    
    # -------------------------------
    # 7. æŒ‰æ¦‚ç‡é™åºæ’åºå¹¶è¿”å›ç»“æœ
    # -------------------------------
    sorted_result = dict(sorted(merged_probs.items(), key=lambda item: item[1], reverse=True))
    return sorted_result

    

def main():
    """Main function for command line interface"""
    start_time = time.perf_counter()
    
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='RXNRECer: A deep learning-based tool for predicting enzyme-catalyzed reactions from protein sequences.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic S1 prediction with TSV output
  rxnrecer -i input.fasta -o output.tsv -m s1
  
  # S2 prediction with JSON output
  rxnrecer -i input.fasta -o output.json -m s2 -f json
  
  # S3 prediction with custom batch size
  rxnrecer -i input.fasta -o output.tsv -m s3 -b 50
  
  # Use default output path (temp directory)
  rxnrecer -i input.fasta -m s1
        """
    )
    
    parser.add_argument('-i', '--input_fasta', type=str, help='Path to input FASTA file (required)')
    parser.add_argument('-o', '--output_file', type=str, default=f'{cfg.TEMP_DIR}res_sample10.tsv', help='Path to output file (default: temp/res_sample10.tsv)')
    parser.add_argument('-f', '--format', type=str, choices=['tsv', 'json'], default='tsv', help='Output format: tsv or json (default: tsv)')
    parser.add_argument('-m', '--mode', type=str, choices=['s1', 's2', 's3'], default='s1', help='Prediction mode: s1 (basic), s2 (detailed), s3 (LLM reasoning) (default: s1)')
    parser.add_argument('-b', '--batch_size', type=int, default=100, help='Batch size for processing (default: 100)')
    parser.add_argument('-v', '--version', action='version', version='RXNRECer 1.2.0')
    
    # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        parser.print_help()
        return
    
    args = parser.parse_args()
    
    # 2. éªŒè¯è¾“å…¥å‚æ•°
    if not args.input_fasta:
        print("Error: Input FASTA file is required!")
        print("Use -i or --input_fasta to specify the input file.")
        print("Use -h or --help for more information.")
        return
    
    if not os.path.exists(args.input_fasta):
        print(f"Error: Input file '{args.input_fasta}' does not exist!")
        return
    
    # 3. å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # 4. æ˜¾ç¤ºè¿è¡Œä¿¡æ¯
    print(f"RXNRECer v1.3.0 - Enzyme Reaction Prediction")
    print(f"Input file: {args.input_fasta}")
    print(f"Output file: {args.output_file}")
    print(f"Output format: {args.format}")
    print(f"Prediction mode: {args.mode}")
    print(f"Batch size: {args.batch_size}")
    print("-" * 50)
    
    # 5. æ£€æŸ¥ç¼“å­˜
    cache_file_name = None
    if cfg.CACHE_ENABLED:
        cache_file_name = ftool.get_cache_filename(input_file=args.input_fasta, mode=args.mode, output_format=args.format)
        if cache_file_name and ftool.check_cache(cache_file_name):
            res = ftool.load_from_cache(cache_file_name)
            if res is not None:
                save_data(res, args.output_file, args.format)
                print(f"âœ… Results loaded from cache and saved to {args.output_file}")
                print(f"â±ï¸  Total time: {time.perf_counter() - start_time:.2f} seconds")
                return 0
    
    # 6. éªŒè¯S3æ¨¡å¼é…ç½®
    if args.mode == 's3' and (not cfg.LLM_API_KEY or not cfg.LLM_API_URL):
        print("Error: LLM API key and URL are required for S3 mode!")
        return
    
    # 7. æ‰§è¡Œé¢„æµ‹
    try:
        res = step_by_step_prediction(
            input_data=args.input_fasta, 
            mode=args.mode, 
            batch_size=args.batch_size
        )
        
        # 8. ä¿å­˜ç»“æœ
        save_data(res, args.output_file, args.format)
        
        # 9. ä¿å­˜åˆ°ç¼“å­˜
        if cfg.CACHE_ENABLED and cache_file_name:
            ftool.save_to_cache(res, cache_file_name)
        
        # 10. å®Œæˆ
        elapsed_time = time.perf_counter() - start_time
        print(f"âœ… Prediction completed successfully!")
        print(f"â±ï¸  Total time: {elapsed_time:.2f} seconds")
        print(f"ğŸ“ Results saved to: {args.output_file}")
        
    except Exception as e:
        print(f"âŒ Error during prediction: {str(e)}")
        print("Please check your input parameters and try again.")
        return 1
    
    return 0

if __name__ == '__main__':
    main()
    
